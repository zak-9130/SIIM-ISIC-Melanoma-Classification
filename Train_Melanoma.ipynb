{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Melanoma.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L023lmsO9K9O"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from util import GradualWarmupSchedulerV2\n",
        "import apex \n",
        "from apex import amp\n",
        "from dataset import get_df, get_transforms, MelanomaDataset\n",
        "from models import Effnet_Melanoma, Resnest_Melanoma, Seresnext_Melanoma\n",
        "\n",
        "\n",
        "\n",
        "############################## Fonction parse_args #####################################################################\n",
        "#                                                                                                                      # \n",
        "#Le programme définit les arguments dont il a besoin et argparse trouvera comment les analyser à partir de sys.argv.   #\n",
        "#Le module argparse génère également automatiquement des messages d'aide                                               #\n",
        "#et d'utilisation et émet des erreurs lorsque les utilisateurs donnent au programme des arguments non valides.         #\n",
        "#                                                                                                                      #   \n",
        "########################################################################################################################\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--kernel-type', type=str, required=True)\n",
        "    parser.add_argument('--data-dir', type=str, default='/raid/')\n",
        "    parser.add_argument('--data-folder', type=int, required=True)\n",
        "    parser.add_argument('--image-size', type=int, required=True)\n",
        "    parser.add_argument('--enet-type', type=str, required=True)\n",
        "    parser.add_argument('--batch-size', type=int, default=64)\n",
        "    parser.add_argument('--num-workers', type=int, default=32)\n",
        "    parser.add_argument('--init-lr', type=float, default=3e-5)\n",
        "    parser.add_argument('--out-dim', type=int, default=9)\n",
        "    parser.add_argument('--n-epochs', type=int, default=15)\n",
        "    parser.add_argument('--use-amp', action='store_true')\n",
        "    parser.add_argument('--use-meta', action='store_true')\n",
        "    parser.add_argument('--DEBUG', action='store_true')\n",
        "    parser.add_argument('--model-dir', type=str, default='./weights')\n",
        "    parser.add_argument('--log-dir', type=str, default='./logs')\n",
        "    parser.add_argument('--CUDA_VISIBLE_DEVICES', type=str, default='0')\n",
        "    parser.add_argument('--fold', type=str, default='0,1,2,3,4')\n",
        "    parser.add_argument('--n-meta-dim', type=str, default='512,128')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "############################## Fonction set_seed #####################################################################\n",
        "#\n",
        "# La performance fait référence au temps d'exécution; CuDNN a plusieurs façons d'implémentations, lorsque cudnn.deterministic est défini sur true.\n",
        "# CuDNN utilise l'heuristique pour le choix de l'implémentation afin d'optimiser les temps de calculs.\n",
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed) # Définit la valeur de départ pour générer des nombres aléatoires. Renvoie un objet torch.Generator.\n",
        "    torch.cuda.manual_seed(seed) \n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True \n",
        "\n",
        "############################## Fonction train_epoch #####################################################################\n",
        "# \n",
        "# train_epoch permet de lancer l'entrainement du model sur differentes partitions \n",
        "# et calculer les probabilité de chaque output\n",
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "def train_epoch(model, loader, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    bar = tqdm(loader)\n",
        "    for (data, target) in bar:  # Faire apparaître un compteur de progression intelligent sur la boucles \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        if args.use_meta:\n",
        "            data, meta = data\n",
        "            data, meta, target = data.to(device), meta.to(device), target.to(device) # Qui déplace un tenseur du CPU ou la mémoire CUDA.\n",
        "            logits = model(data, meta)\n",
        "        else:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            logits = model(data)        \n",
        "        \n",
        "        loss = criterion(logits, target)\n",
        "\n",
        "        if not args.use_amp:\n",
        "            loss.backward()\n",
        "        else:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "\n",
        "        if args.image_size in [896,576]:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_np = loss.detach().cpu().numpy()\n",
        "        train_loss.append(loss_np)\n",
        "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
        "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
        "\n",
        "    train_loss = np.mean(train_loss)\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def get_trans(img, I): # En tenant compte des informations que l'on souhaite sur l'image après une opération on génère une image pivoter (retourner ou miroir)\n",
        "\n",
        "    if I >= 4:\n",
        "        img = img.transpose(2, 3)\n",
        "    if I % 4 == 0:\n",
        "        return img\n",
        "    elif I % 4 == 1:\n",
        "        return img.flip(2)\n",
        "    elif I % 4 == 2:\n",
        "        return img.flip(3)\n",
        "    elif I % 4 == 3:\n",
        "        return img.flip(2).flip(3)\n",
        "\n",
        "\n",
        "def val_epoch(model, loader, mel_idx, is_ext=None, n_test=1, get_output=False):\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "    LOGITS = []\n",
        "    PROBS = []\n",
        "    TARGETS = []\n",
        "    with torch.no_grad():\n",
        "        for (data, target) in tqdm(loader): # Faire apparaître un compteur de progression intelligent sur la boucles\n",
        "            \n",
        "            if args.use_meta:\n",
        "                data, meta = data\n",
        "                data, meta, target = data.to(device), meta.to(device), target.to(device) # Qui déplace un tenseur du CPU ou la mémoire CUDA.\n",
        "                logits = torch.zeros((data.shape[0], args.out_dim)).to(device)# Permet d'initialiser les paramètres\n",
        "                probs = torch.zeros((data.shape[0], args.out_dim)).to(device)# Qui déplace un tenseur du CPU ou la mémoire CUDA.\n",
        "                for I in range(n_test):\n",
        "                    l = model(get_trans(data, I), meta)\n",
        "                    logits += l\n",
        "                    probs += l.softmax(1)\n",
        "            else:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                logits = torch.zeros((data.shape[0], args.out_dim)).to(device)\n",
        "                probs = torch.zeros((data.shape[0], args.out_dim)).to(device)\n",
        "                for I in range(n_test):\n",
        "                    l = model(get_trans(data, I))\n",
        "                    logits += l\n",
        "                    probs += l.softmax(1) #Softmax (rendre non linéaire)= Fonction d'activation \n",
        "            logits /= n_test\n",
        "            probs /= n_test\n",
        "# Garder en mémoire le tenseur (persist) . detach\n",
        "            LOGITS.append(logits.detach().cpu()) # ajoute LOGITS le tenseur séquentiels dans la dimension donnée\n",
        "            PROBS.append(probs.detach().cpu()) # ajoute PROBS le tenseur séquentiels dans la dimension donnée\n",
        "            TARGETS.append(target.detach().cpu()) # ajoute TARGETS le tenseur séquentiels dans la dimension donnée\n",
        "\n",
        "            loss = criterion(logits, target)\n",
        "            val_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    val_loss = np.mean(val_loss)\n",
        "    LOGITS = torch.cat(LOGITS).numpy() # Concatène LOGITS le tenseur séquentiels dans la dimension donnée\n",
        "    PROBS = torch.cat(PROBS).numpy() # Concatène PROBS le tenseur séquentiels dans la dimension donnée\n",
        "    TARGETS = torch.cat(TARGETS).numpy() # Concatène TARGETS le tenseur séquentiels dans la dimension donnée\n",
        "\n",
        "    if get_output:\n",
        "        return LOGITS, PROBS\n",
        "    else:\n",
        "        acc = (PROBS.argmax(1) == TARGETS).mean() * 100.\n",
        "        auc = roc_auc_score((TARGETS == mel_idx).astype(float), PROBS[:, mel_idx])\n",
        "        auc_20 = roc_auc_score((TARGETS[is_ext == 0] == mel_idx).astype(float), PROBS[is_ext == 0, mel_idx])\n",
        "        return val_loss, acc, auc, auc_20\n",
        "\n",
        "############################################ fonction run ###########################################################@\n",
        "#\n",
        "# Cette fonction prend comme argument le dataframe, les partitions, les datas et les indices des images de melanomia\n",
        "# permet de lancer l'algorithme afin lancer le modèle, \n",
        "#\n",
        "#######################################################################################################################\n",
        "\n",
        "\n",
        "def run(fold, df, meta_features, n_meta_features, transforms_train, transforms_val, mel_idx):\n",
        "\n",
        "    if args.DEBUG:\n",
        "        args.n_epochs = 5\n",
        "        df_train = df[df['fold'] != fold].sample(args.batch_size * 5)\n",
        "        df_valid = df[df['fold'] == fold].sample(args.batch_size * 5)\n",
        "    else:\n",
        "        df_train = df[df['fold'] != fold]\n",
        "        df_valid = df[df['fold'] == fold]\n",
        "\n",
        "    dataset_train = MelanomaDataset(df_train, 'train', meta_features, transform=transforms_train)\n",
        "    dataset_valid = MelanomaDataset(df_valid, 'valid', meta_features, transform=transforms_val)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=args.batch_size, sampler=RandomSampler(dataset_train), num_workers=args.num_workers)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=args.batch_size, num_workers=args.num_workers)\n",
        "\n",
        "    model = ModelClass(\n",
        "        args.enet_type,\n",
        "        n_meta_features=n_meta_features,\n",
        "        n_meta_dim=[int(nd) for nd in args.n_meta_dim.split(',')],\n",
        "        out_dim=args.out_dim,\n",
        "        pretrained=True\n",
        "    )\n",
        "    if DP:\n",
        "        model = apex.parallel.convert_syncbn_model(model)\n",
        "    model = model.to(device)\n",
        "\n",
        "    auc_max = 0.\n",
        "    auc_20_max = 0.\n",
        "    model_file  = os.path.join(args.model_dir, f'{args.kernel_type}_best_fold{fold}.pth')\n",
        "    model_file2 = os.path.join(args.model_dir, f'{args.kernel_type}_best_20_fold{fold}.pth')\n",
        "    model_file3 = os.path.join(args.model_dir, f'{args.kernel_type}_final_fold{fold}.pth')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.init_lr)\n",
        "    if args.use_amp:\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "    if DP:\n",
        "        model = nn.DataParallel(model)\n",
        "#     scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.n_epochs - 1)\n",
        "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.n_epochs - 1)\n",
        "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
        "    \n",
        "    print(len(dataset_train), len(dataset_valid))\n",
        "\n",
        "    for epoch in range(1, args.n_epochs + 1):\n",
        "        print(time.ctime(), f'Fold {fold}, Epoch {epoch}')\n",
        "#         scheduler_warmup.step(epoch - 1)\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer)\n",
        "        val_loss, acc, auc, auc_20 = val_epoch(model, valid_loader, mel_idx, is_ext=df_valid['is_ext'].values)\n",
        "\n",
        "        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {(val_loss):.5f}, acc: {(acc):.4f}, auc: {(auc):.6f}, auc_20: {(auc_20):.6f}.'\n",
        "        print(content)\n",
        "        with open(os.path.join(args.log_dir, f'log_{args.kernel_type}.txt'), 'a') as appender:\n",
        "            appender.write(content + '\\n')\n",
        "\n",
        "        scheduler_warmup.step()    \n",
        "        if epoch==2: scheduler_warmup.step() # bug workaround   \n",
        "            \n",
        "        if auc > auc_max:\n",
        "            print('auc_max ({:.6f} --> {:.6f}). Saving model ...'.format(auc_max, auc))\n",
        "            torch.save(model.state_dict(), model_file)\n",
        "            auc_max = auc\n",
        "        if auc_20 > auc_20_max:\n",
        "            print('auc_20_max ({:.6f} --> {:.6f}). Saving model ...'.format(auc_20_max, auc_20))\n",
        "            torch.save(model.state_dict(), model_file2)\n",
        "            auc_20_max = auc_20\n",
        "\n",
        "    torch.save(model.state_dict(), model_file3)\n",
        "    \n",
        "############################################################# fonction Main ###################################################\n",
        "# \n",
        "# La fonction principale est la fonction principale des programmes\n",
        "# L'exécution d'un programme entraîné automatiquement l'appel de la fonction principale. ici le main fait appel à get_df et get transform\n",
        "# cette fonction principale permet de créer le dataset apres Data Augmentation pour ameliorer l'entrainement du modèl.\n",
        "#\n",
        "#\n",
        "###############################################################################################################################\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    df, df_test, meta_features, n_meta_features, mel_idx = get_df(\n",
        "        args.kernel_type,\n",
        "        args.out_dim,\n",
        "        args.data_dir,\n",
        "        args.data_folder,\n",
        "        args.use_meta\n",
        "    )\n",
        "\n",
        "    transforms_train, transforms_val = get_transforms(args.image_size)\n",
        "\n",
        "    folds = [int(i) for i in args.fold.split(',')]\n",
        "    for fold in folds:\n",
        "        run(fold, df, meta_features, n_meta_features, transforms_train, transforms_val, mel_idx)\n",
        "\n",
        "######################################################## Condition de lancement du program Main#########################################################################         \n",
        "#\n",
        "# Cette condition est utilisée pour développer un module pouvant à la fois être exécuté directement mais aussi être importé un autre module pour apporter ses fonctions.\n",
        "# Vous pouvez insérer dans ce bloc de code des instructions destinées au cas où le module est directement exécuté.\n",
        "# \n",
        "###################################################################################################################################################################### \n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = parse_args() # recuperation des arguments utilise pour lanecr l'entrainement\n",
        "    os.makedirs(args.model_dir, exist_ok=True) # os.makedirs () créer tous les répertoires manquants dans le chemin spécifié (model_dir)\n",
        "    os.makedirs(args.log_dir, exist_ok=True)\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = args.CUDA_VISIBLE_DEVICES # Il renvoie un dictionnaire ayant la variable d'environnement CUDA pour le calcul en GPU\n",
        "\n",
        "#### Condition permettant de choisir quel modele utilisé ( \"Enet type\") #############################\n",
        "    if args.enet_type == 'resnest101': \n",
        "        ModelClass = Resnest_Melanoma\n",
        "    elif args.enet_type == 'seresnext101':\n",
        "        ModelClass = Seresnext_Melanoma\n",
        "    elif 'efficientnet' in args.enet_type:\n",
        "        ModelClass = Effnet_Melanoma\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    DP = len(os.environ['CUDA_VISIBLE_DEVICES']) > 1\n",
        "\n",
        "    set_seed()\n",
        "\n",
        "    device = torch.device('cuda') ### Calcul en GPU\n",
        "    criterion = nn.CrossEntropyLoss() # critere d'évaluation cross entropy\n",
        "\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Dc6pYePqT4"
      },
      "source": [
        "\n",
        "########################## Get_dF ################################################\n",
        "# Fonction permettant de recuperer une dataset filtree par type de donné\n",
        "# pour l'entrainement du modèl                                                                                                     ##\n",
        "##################################################################################\n",
        "\n",
        "\n",
        "def get_df(kernel_type, out_dim, data_dir, data_folder, use_meta):\n",
        "\n",
        "    # 2020 data\n",
        "    df_train = pd.read_csv(os.path.join(data_dir, f'jpeg-melanoma-{data_folder}x{data_folder}', 'train.csv'))\n",
        "    df_train = df_train[df_train['tfrecord'] != -1].reset_index(drop=True) #retire les index des toutes les colonnes où df_train['tfrecord'] # -1\n",
        "    df_train['filepath'] = df_train['image_name'].apply(lambda x: os.path.join(data_dir, f'jpeg-melanoma-{data_folder}x{data_folder}/train', f'{x}.jpg'))\n",
        "# création dictionnaire de valeur pour partitionner les données df\n",
        "    if 'newfold' in kernel_type:\n",
        "        tfrecord2fold = {\n",
        "            8:0, 5:0, 11:0,\n",
        "            7:1, 0:1, 6:1,\n",
        "            10:2, 12:2, 13:2,\n",
        "            9:3, 1:3, 3:3,\n",
        "            14:4, 2:4, 4:4,\n",
        "        }\n",
        "    elif 'oldfold' in kernel_type:\n",
        "        tfrecord2fold = {i: i % 5 for i in range(15)}\n",
        "    else:\n",
        "        tfrecord2fold = {\n",
        "            2:0, 4:0, 5:0,\n",
        "            1:1, 10:1, 13:1,\n",
        "            0:2, 9:2, 12:2,\n",
        "            3:3, 8:3, 11:3,\n",
        "            6:4, 7:4, 14:4,\n",
        "        }\n",
        "    df_train['fold'] = df_train['tfrecord'].map(tfrecord2fold)\n",
        "    df_train['is_ext'] = 0\n",
        "\n",
        "    # 2018, 2019 data (external data)\n",
        "    df_train2 = pd.read_csv(os.path.join(data_dir, f'jpeg-isic2019-{data_folder}x{data_folder}', 'train.csv'))\n",
        "    df_train2 = df_train2[df_train2['tfrecord'] >= 0].reset_index(drop=True)\n",
        "    df_train2['filepath'] = df_train2['image_name'].apply(lambda x: os.path.join(data_dir, f'jpeg-isic2019-{data_folder}x{data_folder}/train', f'{x}.jpg'))  #   stocke dans filepath les fichier.jpg dans image_name\n",
        "    if 'newfold' in kernel_type:\n",
        "        df_train2['tfrecord'] = df_train2['tfrecord'] % 15\n",
        "        df_train2['fold'] = df_train2['tfrecord'].map(tfrecord2fold)\n",
        "    else:\n",
        "        df_train2['fold'] = df_train2['tfrecord'] % 5\n",
        "    df_train2['is_ext'] = 1\n",
        "\n",
        "##############   BKL= benign keratosis ###########\n",
        "##############   diagnosis - informations détaillées sur le diagnostic (train uniquement) #############\n",
        "##############   changement de nom de variable dans le fichier d'entrainement dans la colonne diagnosis ###############\n",
        "    # Preprocess Target\n",
        "    df_train['diagnosis']  = df_train['diagnosis'].apply(lambda x: x.replace('seborrheic keratosis', 'BKL'))\n",
        "    df_train['diagnosis']  = df_train['diagnosis'].apply(lambda x: x.replace('lichenoid keratosis', 'BKL'))\n",
        "    df_train['diagnosis']  = df_train['diagnosis'].apply(lambda x: x.replace('solar lentigo', 'BKL'))\n",
        "    df_train['diagnosis']  = df_train['diagnosis'].apply(lambda x: x.replace('lentigo NOS', 'BKL'))\n",
        "    df_train['diagnosis']  = df_train['diagnosis'].apply(lambda x: x.replace('cafe-au-lait macule', 'unknown'))\n",
        "    df_train['diagnosis']  = df_train['diagnosis'].apply(lambda x: x.replace('atypical melanocytic proliferation', 'unknown'))\n",
        "\n",
        "    if out_dim == 9:\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('NV', 'nevus'))\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('MEL', 'melanoma'))\n",
        "    elif out_dim == 4:\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('NV', 'nevus'))\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('MEL', 'melanoma'))\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('DF', 'unknown'))\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('AK', 'unknown'))\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('SCC', 'unknown'))\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('VASC', 'unknown'))\n",
        "        df_train2['diagnosis'] = df_train2['diagnosis'].apply(lambda x: x.replace('BCC', 'unknown'))\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # concat train data\n",
        "    df_train = pd.concat([df_train, df_train2]).reset_index(drop=True)\n",
        "\n",
        "    # test data\n",
        "    df_test = pd.read_csv(os.path.join(data_dir, f'jpeg-melanoma-{data_folder}x{data_folder}', 'test.csv'))\n",
        "    df_test['filepath'] = df_test['image_name'].apply(lambda x: os.path.join(data_dir, f'jpeg-melanoma-{data_folder}x{data_folder}/test', f'{x}.jpg'))\n",
        "\n",
        "    if use_meta:\n",
        "        df_train, df_test, meta_features, n_meta_features = get_meta_data(df_train, df_test)\n",
        "    else:\n",
        "        meta_features = None\n",
        "        n_meta_features = 0\n",
        "\n",
        "    # class mapping\n",
        "    diagnosis2idx = {d: idx for idx, d in enumerate(sorted(df_train.diagnosis.unique()))} # Formation d'un dictionnaire  \n",
        "\n",
        "\n",
        "    df_train['target'] = df_train['diagnosis'].map(diagnosis2idx) # Création de la colonne target dans df train en utilisant l'iteration précédente\n",
        "    mel_idx = diagnosis2idx['melanoma'] # Stockage de melanoma qui se trouve dans diagnosis2idx\n",
        "\n",
        "    return df_train, df_test, meta_features, n_meta_features, mel_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiYlTrwMQDPB"
      },
      "source": [
        "################################## fonction get_transforms  ##################################################\n",
        "##                                                                              \n",
        "## Utilisation de la librairie Albumentation pour transformer les image et creer de nouvelles features.     # \n",
        "##                                                                                                          # \n",
        "## ##########################################################################################################\n",
        "\n",
        "def get_transforms(image_size):\n",
        "    ## Création d'un dataset ( Matrice )\n",
        "    transforms_train = albumentations.Compose([\n",
        "        albumentations.Transpose(p=0.5), # Trasnpostion d'une matrice\n",
        "        albumentations.VerticalFlip(p=0.5), # Retournement vertical de chaque vecteur \n",
        "        albumentations.HorizontalFlip(p=0.5), # Retournement horizontale de chaque vecteur \n",
        "        albumentations.RandomBrightness(limit=0.2, p=0.75), # Retournement horizontale de chaque vecteur \n",
        "        albumentations.RandomContrast(limit=0.2, p=0.75),\n",
        "        albumentations.OneOf([ \n",
        "            albumentations.MotionBlur(blur_limit=5),\n",
        "            albumentations.MedianBlur(blur_limit=5),\n",
        "            albumentations.GaussianBlur(blur_limit=5),\n",
        "            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.OneOf([\n",
        "            albumentations.OpticalDistortion(distort_limit=1.0),\n",
        "            albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n",
        "            albumentations.ElasticTransform(alpha=3),\n",
        "        ], p=0.7),\n",
        "\n",
        "        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n",
        "        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
        "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
        "        albumentations.Resize(image_size, image_size),\n",
        "        albumentations.Cutout(max_h_size=int(image_size * 0.375), max_w_size=int(image_size * 0.375), num_holes=1, p=0.7),\n",
        "        albumentations.Normalize()\n",
        "    ])\n",
        "\n",
        "    transforms_val = albumentations.Compose([\n",
        "        albumentations.Resize(image_size, image_size),\n",
        "        albumentations.Normalize()\n",
        "    ])\n",
        "\n",
        "    return transforms_train, transforms_val"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}